# VOICE WORKFLOW IMPLEMENTATION - BUILD NOW

## ðŸš¨ IMMEDIATE TASK: Replace Realtime API with OpenAI Assistants

Your current voice workflow uses OpenAI Realtime API. We're migrating to OpenAI Assistants for better patient context and persistent memory. Build this EXACT replacement following the current sequence.

---

## CURRENT VOICE WORKFLOW SEQUENCE (PRESERVE THIS EXACTLY)

1. **User clicks Record** â†’ Audio capture starts
2. **Real-time transcription** â†’ Text streams to interface  
3. **Role-based suggestions** â†’ AI prompts appear during recording
4. **User stops recording** â†’ Complete transcription sent for processing
5. **AI processes voice** â†’ SOAP note + orders + billing codes generated (60-90 seconds)
6. **Chart updates** â†’ All medical data refreshes automatically

---

## STEP 1: CREATE OpenAI Assistant Service

```typescript
// services/assistant-service.ts
import OpenAI from 'openai';

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

export class AssistantService {
  private assistantId: string;
  
  constructor() {
    // Create assistant with medical expertise
    this.assistantId = process.env.OPENAI_ASSISTANT_ID || '';
  }
  
  async initializeAssistant() {
    if (!this.assistantId) {
      const assistant = await openai.beta.assistants.create({
        name: "Medical Documentation Assistant",
        instructions: `You are an expert medical AI assistant that processes voice recordings from healthcare providers.

CORE RESPONSIBILITIES:
1. Convert voice transcriptions into structured SOAP notes
2. Generate appropriate medical orders based on clinical content
3. Suggest relevant CPT codes for billing
4. Provide role-based suggestions (different for nurses vs providers)
5. Update patient charts using smart update strategy

SMART UPDATE RULES:
- Historical data (family history, social history, allergies): UPDATE existing records
- Factual data (vitals, medications, labs, diagnoses): APPEND as new records
- Never modify existing vital signs, lab results, or medication records

ROLE-BASED PROMPTING:
- For NURSES: Focus on assessment questions, patient education, symptom clarification
- For PROVIDERS: Focus on differential diagnosis, treatment plans, clinical decision support

OUTPUT FORMAT: Always respond with valid JSON containing:
{
  "soapNote": {
    "subjective": "string",
    "objective": "string", 
    "assessment": "string",
    "plan": "string"
  },
  "suggestions": {
    "realTimePrompts": ["suggestion1", "suggestion2"],
    "clinicalGuidance": "string"
  },
  "chartUpdates": {
    "historicalUpdates": {
      "familyHistory": [{"familyMember": "string", "medicalHistory": "string"}],
      "socialHistory": [{"category": "string", "currentStatus": "string"}],
      "allergies": [{"allergen": "string", "reaction": "string", "severity": "string"}]
    },
    "factualAppends": {
      "vitals": [{"measuredAt": "datetime", "systolicBp": number, ...}],
      "medications": [{"medicationName": "string", "dosage": "string", ...}],
      "diagnoses": [{"diagnosis": "string", "icd10Code": "string", ...}]
    }
  },
  "draftOrders": [
    {
      "orderType": "lab|imaging|medication|referral",
      "orderDetails": "string",
      "priority": "routine|urgent|stat",
      "clinicalIndication": "string"
    }
  ],
  "cptCodes": [
    {
      "code": "string",
      "description": "string",
      "units": number,
      "modifier": "string"
    }
  ]
}`,
        model: "gpt-4o",
        tools: []
      });
      this.assistantId = assistant.id;
    }
  }
  
  async getOrCreateThread(patientId: number): Promise<string> {
    // Check if patient already has a thread
    const patient = await db.select().from(patients).where(eq(patients.id, patientId)).limit(1);
    
    if (patient[0]?.assistantThreadId) {
      return patient[0].assistantThreadId;
    }
    
    // Create new thread
    const thread = await openai.beta.threads.create();
    
    // Save thread ID to patient record
    await db.update(patients)
      .set({ assistantThreadId: thread.id })
      .where(eq(patients.id, patientId));
    
    return thread.id;
  }
  
  async processVoiceRecording(
    threadId: string,
    transcription: string,
    patientId: number,
    encounterId: number,
    userRole: string,
    currentChartData: any
  ) {
    // Add message to thread with full context
    await openai.beta.threads.messages.create(threadId, {
      role: "user",
      content: `
VOICE RECORDING TRANSCRIPTION: ${transcription}

CURRENT PATIENT CHART: ${JSON.stringify(currentChartData)}

USER ROLE: ${userRole}

ENCOUNTER ID: ${encounterId}

Please process this voice recording and provide structured medical documentation following the output format specified in your instructions.
      `
    });
    
    // Run assistant
    const run = await openai.beta.threads.runs.create(threadId, {
      assistant_id: this.assistantId
    });
    
    // Wait for completion
    let runStatus = await openai.beta.threads.runs.retrieve(threadId, run.id);
    while (runStatus.status === 'in_progress' || runStatus.status === 'queued') {
      await new Promise(resolve => setTimeout(resolve, 1000));
      runStatus = await openai.beta.threads.runs.retrieve(threadId, run.id);
    }
    
    if (runStatus.status === 'completed') {
      const messages = await openai.beta.threads.messages.list(threadId);
      const lastMessage = messages.data[0];
      
      if (lastMessage.content[0].type === 'text') {
        return JSON.parse(lastMessage.content[0].text.value);
      }
    }
    
    throw new Error('Assistant processing failed');
  }
}
```

---

## STEP 2: CREATE Voice Chart Updater

```typescript
// services/voice-chart-updater.ts
import { AssistantService } from './assistant-service.js';
import { db } from '../db/index.js';
import { encounters, patients, familyHistory, socialHistory, allergies, vitals, medications, diagnoses } from '../db/schema.js';
import { eq } from 'drizzle-orm';

export class VoiceChartUpdater {
  private assistantService: AssistantService;
  
  constructor() {
    this.assistantService = new AssistantService();
  }
  
  async processVoiceRecording(
    transcription: string,
    patientId: number,
    encounterId: number,
    userRole: string
  ) {
    console.log('ðŸŽ¤ Processing voice recording for patient', patientId);
    
    // Get current chart data
    const currentChart = await this.getCurrentChartData(patientId);
    
    // Get or create assistant thread
    const threadId = await this.assistantService.getOrCreateThread(patientId);
    
    // Process with AI Assistant
    const aiResponse = await this.assistantService.processVoiceRecording(
      threadId,
      transcription,
      patientId,
      encounterId,
      userRole,
      currentChart
    );
    
    // Apply all updates to database
    await this.applyChartUpdates(aiResponse, patientId, encounterId);
    
    // Update encounter with processed data
    await this.updateEncounter(encounterId, aiResponse, transcription);
    
    console.log('âœ… Voice processing complete');
    return aiResponse;
  }
  
  private async getCurrentChartData(patientId: number) {
    const [
      patientData,
      familyHistoryData,
      socialHistoryData,
      allergiesData,
      vitalsData,
      medicationsData,
      diagnosesData
    ] = await Promise.all([
      db.select().from(patients).where(eq(patients.id, patientId)).limit(1),
      db.select().from(familyHistory).where(eq(familyHistory.patientId, patientId)),
      db.select().from(socialHistory).where(eq(socialHistory.patientId, patientId)),
      db.select().from(allergies).where(eq(allergies.patientId, patientId)),
      db.select().from(vitals).where(eq(vitals.patientId, patientId)).orderBy(desc(vitals.measuredAt)).limit(10),
      db.select().from(medications).where(eq(medications.patientId, patientId)).where(eq(medications.status, 'active')),
      db.select().from(diagnoses).where(eq(diagnoses.patientId, patientId)).where(eq(diagnoses.status, 'active'))
    ]);
    
    return {
      patient: patientData[0],
      familyHistory: familyHistoryData,
      socialHistory: socialHistoryData,
      allergies: allergiesData,
      recentVitals: vitalsData,
      currentMedications: medicationsData,
      activeDiagnoses: diagnosesData
    };
  }
  
  private async applyChartUpdates(aiResponse: any, patientId: number, encounterId: number) {
    const { chartUpdates } = aiResponse;
    
    // HISTORICAL UPDATES (can modify existing records)
    if (chartUpdates.historicalUpdates) {
      await this.updateHistoricalData(chartUpdates.historicalUpdates, patientId, encounterId);
    }
    
    // FACTUAL APPENDS (always create new records)
    if (chartUpdates.factualAppends) {
      await this.appendFactualData(chartUpdates.factualAppends, patientId, encounterId);
    }
  }
  
  private async updateHistoricalData(updates: any, patientId: number, encounterId: number) {
    // Family History - UPSERT (update or insert)
    if (updates.familyHistory) {
      for (const family of updates.familyHistory) {
        await db.insert(familyHistory)
          .values({
            patientId,
            familyMember: family.familyMember,
            medicalHistory: family.medicalHistory,
            lastUpdatedEncounter: encounterId
          })
          .onConflictDoUpdate({
            target: [familyHistory.patientId, familyHistory.familyMember],
            set: {
              medicalHistory: family.medicalHistory,
              lastUpdatedEncounter: encounterId,
              updatedAt: new Date()
            }
          });
      }
    }
    
    // Social History - UPSERT
    if (updates.socialHistory) {
      for (const social of updates.socialHistory) {
        await db.insert(socialHistory)
          .values({
            patientId,
            category: social.category,
            currentStatus: social.currentStatus,
            historyNotes: social.historyNotes,
            lastUpdatedEncounter: encounterId
          })
          .onConflictDoUpdate({
            target: [socialHistory.patientId, socialHistory.category],
            set: {
              currentStatus: social.currentStatus,
              historyNotes: social.historyNotes,
              lastUpdatedEncounter: encounterId,
              updatedAt: new Date()
            }
          });
      }
    }
    
    // Allergies - UPSERT
    if (updates.allergies) {
      for (const allergy of updates.allergies) {
        await db.insert(allergies)
          .values({
            patientId,
            allergen: allergy.allergen,
            reaction: allergy.reaction,
            severity: allergy.severity,
            lastUpdatedEncounter: encounterId
          })
          .onConflictDoUpdate({
            target: [allergies.patientId, allergies.allergen],
            set: {
              reaction: allergy.reaction,
              severity: allergy.severity,
              lastUpdatedEncounter: encounterId,
              updatedAt: new Date()
            }
          });
      }
    }
  }
  
  private async appendFactualData(appends: any, patientId: number, encounterId: number) {
    // Vitals - ALWAYS INSERT NEW RECORDS
    if (appends.vitals) {
      for (const vital of appends.vitals) {
        await db.insert(vitals).values({
          patientId,
          encounterId,
          measuredAt: new Date(vital.measuredAt),
          systolicBp: vital.systolicBp,
          diastolicBp: vital.diastolicBp,
          heartRate: vital.heartRate,
          temperature: vital.temperature,
          weight: vital.weight,
          height: vital.height,
          recordedBy: 'AI Assistant'
        });
      }
    }
    
    // Medications - ALWAYS INSERT NEW RECORDS
    if (appends.medications) {
      for (const med of appends.medications) {
        await db.insert(medications).values({
          patientId,
          encounterId,
          medicationName: med.medicationName,
          dosage: med.dosage,
          frequency: med.frequency,
          route: med.route,
          startDate: new Date(med.startDate),
          endDate: med.endDate ? new Date(med.endDate) : null,
          status: med.status || 'active',
          prescriber: 'AI Assistant',
          medicalProblem: med.medicalProblem
        });
      }
    }
    
    // Diagnoses - ALWAYS INSERT NEW RECORDS
    if (appends.diagnoses) {
      for (const diagnosis of appends.diagnoses) {
        await db.insert(diagnoses).values({
          patientId,
          encounterId,
          diagnosis: diagnosis.diagnosis,
          icd10Code: diagnosis.icd10Code,
          diagnosisDate: new Date(),
          status: diagnosis.status || 'active',
          notes: diagnosis.notes
        });
      }
    }
  }
  
  private async updateEncounter(encounterId: number, aiResponse: any, transcription: string) {
    await db.update(encounters)
      .set({
        transcriptionRaw: transcription,
        transcriptionProcessed: transcription,
        subjective: aiResponse.soapNote.subjective,
        objective: aiResponse.soapNote.objective,
        assessment: aiResponse.soapNote.assessment,
        plan: aiResponse.soapNote.plan,
        aiSuggestions: aiResponse.suggestions,
        draftOrders: aiResponse.draftOrders,
        cptCodes: aiResponse.cptCodes,
        lastChartUpdate: new Date(),
        updatedAt: new Date()
      })
      .where(eq(encounters.id, encounterId));
  }
}
```

---

## STEP 3: CREATE API Endpoint

```typescript
// routes/voice-routes.ts
import { VoiceChartUpdater } from '../services/voice-chart-updater.js';

const voiceUpdater = new VoiceChartUpdater();

app.post('/api/encounters/:encounterId/voice-update', async (req, res) => {
  try {
    const { encounterId } = req.params;
    const { transcription, patientId, userRole } = req.body;
    
    console.log('ðŸŽ¤ Voice update request:', { encounterId, patientId, userRole });
    
    const result = await voiceUpdater.processVoiceRecording(
      transcription,
      parseInt(patientId),
      parseInt(encounterId),
      userRole
    );
    
    res.json({
      success: true,
      soapNote: result.soapNote,
      suggestions: result.suggestions,
      draftOrders: result.draftOrders,
      cptCodes: result.cptCodes,
      chartUpdated: true
    });
  } catch (error) {
    console.error('Voice processing error:', error);
    res.status(500).json({ 
      success: false, 
      error: 'Voice processing failed',
      details: error.message 
    });
  }
});
```

---

## STEP 4: BUILD Voice Recording Component

```tsx
// components/VoiceRecorder.tsx
import { useState } from 'react';
import { Button } from '@/components/ui/button';
import { Mic, MicOff, Loader2 } from 'lucide-react';

interface VoiceRecorderProps {
  encounterId: number;
  patientId: number;
  userRole: string;
  onProcessingComplete: (result: any) => void;
}

export function VoiceRecorder({ encounterId, patientId, userRole, onProcessingComplete }: VoiceRecorderProps) {
  const [isRecording, setIsRecording] = useState(false);
  const [isProcessing, setIsProcessing] = useState(false);
  const [transcription, setTranscription] = useState('');
  const [mediaRecorder, setMediaRecorder] = useState<MediaRecorder | null>(null);
  
  const startRecording = async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      const recorder = new MediaRecorder(stream);
      const audioChunks: BlobPart[] = [];
      
      recorder.ondataavailable = (event) => {
        audioChunks.push(event.data);
      };
      
      recorder.onstop = async () => {
        const audioBlob = new Blob(audioChunks, { type: 'audio/wav' });
        
        // Convert audio to text using your existing transcription service
        // Or integrate with OpenAI Whisper API
        const transcriptionText = await transcribeAudio(audioBlob);
        setTranscription(transcriptionText);
        
        // Process with voice chart updater
        await processVoiceRecording(transcriptionText);
      };
      
      recorder.start();
      setMediaRecorder(recorder);
      setIsRecording(true);
    } catch (error) {
      console.error('Failed to start recording:', error);
    }
  };
  
  const stopRecording = () => {
    if (mediaRecorder) {
      mediaRecorder.stop();
      mediaRecorder.stream.getTracks().forEach(track => track.stop());
      setIsRecording(false);
    }
  };
  
  const processVoiceRecording = async (transcription: string) => {
    setIsProcessing(true);
    
    try {
      const response = await fetch(`/api/encounters/${encounterId}/voice-update`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          transcription,
          patientId,
          userRole
        })
      });
      
      const result = await response.json();
      
      if (result.success) {
        onProcessingComplete(result);
        console.log('âœ… Voice processing complete:', result);
      } else {
        console.error('Voice processing failed:', result.error);
      }
    } catch (error) {
      console.error('API error:', error);
    } finally {
      setIsProcessing(false);
    }
  };
  
  const transcribeAudio = async (audioBlob: Blob): Promise<string> => {
    // Implement your transcription logic here
    // You can use OpenAI Whisper API or your existing transcription service
    const formData = new FormData();
    formData.append('audio', audioBlob);
    
    const response = await fetch('/api/transcribe', {
      method: 'POST',
      body: formData
    });
    
    const result = await response.json();
    return result.text;
  };
  
  return (
    <div className="flex flex-col items-center space-y-4 p-4 border rounded-lg">
      <div className="flex items-center space-x-4">
        <Button
          onClick={isRecording ? stopRecording : startRecording}
          disabled={isProcessing}
          variant={isRecording ? "destructive" : "default"}
          size="lg"
        >
          {isProcessing ? (
            <Loader2 className="h-6 w-6 animate-spin" />
          ) : isRecording ? (
            <MicOff className="h-6 w-6" />
          ) : (
            <Mic className="h-6 w-6" />
          )}
          {isProcessing ? 'Processing...' : isRecording ? 'Stop Recording' : 'Start Recording'}
        </Button>
      </div>
      
      {transcription && (
        <div className="w-full p-3 bg-gray-50 rounded border">
          <p className="text-sm font-medium mb-2">Transcription:</p>
          <p className="text-sm">{transcription}</p>
        </div>
      )}
      
      {isProcessing && (
        <div className="text-center">
          <p className="text-sm text-gray-600">
            AI is processing your recording and updating the chart...
          </p>
          <p className="text-xs text-gray-500">This usually takes 60-90 seconds</p>
        </div>
      )}
    </div>
  );
}
```

---

## ENVIRONMENT VARIABLES NEEDED

Add these to your `.env` file:

```env
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_ASSISTANT_ID=your_assistant_id_here
```

---

## IMMEDIATE NEXT STEPS

1. **CREATE** the AssistantService class
2. **BUILD** the VoiceChartUpdater service  
3. **ADD** the voice API endpoint
4. **IMPLEMENT** the VoiceRecorder component
5. **TEST** with a real patient encounter

This preserves your exact current workflow while upgrading from Realtime API to Assistants for better patient memory and context!