import React, { useState, useCallback } from "react";
import { WebSocketClient } from "../utils/WebSocketClient";
import {
  Dialog,
  DialogContent,
  DialogDescription,
  DialogHeader,
  DialogTitle,
  DialogTrigger,
  DialogFooter,
} from "@/components/ui/dialog";
import { Button } from "@/components/ui/button";
import { Mic, Square, MessageSquare, Send } from "lucide-react";
import { cn } from "@/lib/utils";
import { Textarea } from "@/components/ui/textarea";
import { formatPatientChartForGpt } from "../utils/formatPatientChartForGpt";
import { useToast } from "@/hooks/use-toast";

interface AudioRecorderProps {
  client: WebSocketClient | null;
  onStopRecording: () => void;
  onStartRecording?: () => void; // Optional callback for when recording starts
  patientId: string | number; // Accept both string and number
  handleRecordingTriger: () => void;
}

interface AudioConfig {
  sampleRate: number;
  channelCount: number;
  bufferSize: number;
}

// Flag to track if we've shown the fallback recommendation
let hasSuggestedTextFallback = false;

// Use more browser-friendly audio settings to improve compatibility
const AUDIO_CONFIG: AudioConfig = {
  sampleRate: 16000, // Standard sample rate widely supported
  channelCount: 1, // Mono audio is simpler to process
  bufferSize: 4096, // Smaller buffer size for better compatibility
};

export const AudioRecorder: React.FC<AudioRecorderProps> = ({
  client,
  onStopRecording,
  onStartRecording,
  patientId,
  handleRecordingTriger,
}) => {
  const { toast } = useToast();
  const [isRecording, setIsRecording] = useState<boolean>(false);
  const [showConfirmDialog, setShowConfirmDialog] = useState<boolean>(false);
  const [audioContext, setAudioContext] = useState<AudioContext | null>(null);
  const [mediaStream, setMediaStream] = useState<MediaStream | null>(null);
  const [isPaused, setIsPaused] = useState<boolean>(false);
  const [audioProcessor, setAudioProcessor] =
    useState<ScriptProcessorNode | null>(null);
  const [audioSource, setAudioSource] =
    useState<MediaStreamAudioSourceNode | null>(null);
  const [textInput, setTextInput] = useState<string>("");

  // Don't default to text mode, always try to use audio recording first
  const [showTextMode, setShowTextMode] = useState<boolean>(false);

  const processAudioData = useCallback(
    (inputData: Float32Array, sampleRate: number, channels: number) => {
      const pcm16Data = new Int16Array(inputData.length);
      for (let i = 0; i < inputData.length; i++) {
        const sample = Math.max(-1, Math.min(1, inputData[i]));
        pcm16Data[i] = sample < 0 ? sample * 0x8000 : sample * 0x7fff;
      }
      return new Blob([pcm16Data.buffer], { type: "audio/raw" });
    },
    [],
  );
  //*** the most likely source of the trouble regarding token usage ANDREW

  // Check if we're in a secure context (required for mediaDevices API)
  const isSecureContext = () => {
    return window.isSecureContext === true;
  };

  // Check if we're in a Replit iframe environment
  const isReplitIframe = () => {
    try {
      // Check if hostname contains replit domain
      const isReplitDomain = window.location.hostname.includes("replit");

      // Check if we're in an iframe
      let isIframe = false;
      try {
        isIframe = window !== window.top;
      } catch (e) {
        // If we can't access window.top due to security restrictions,
        // we're definitely in a cross-origin iframe
        isIframe = true;
      }

      console.log(
        `[AudioRecorder] Environment check: isReplitDomain=${isReplitDomain}, isIframe=${isIframe}`,
      );
      return isReplitDomain && isIframe;
    } catch (e) {
      console.error(
        "[AudioRecorder] Error checking Replit iframe environment:",
        e,
      );
      // If there's an error checking, assume we're not in a Replit iframe
      // to avoid unnecessary warnings
      return false;
    }
  };

  // Check if the mediaDevices API is available
  const hasMediaDevices = () => {
    return !!(navigator.mediaDevices && navigator.mediaDevices.getUserMedia);
  };

  // Helper function to display a modal error with specific instructions
  const showPermissionErrorModal = (errorType: string) => {
    let title = "Microphone Error";
    let message =
      "An unknown error occurred when requesting microphone access.";

    switch (errorType) {
      case "NotAllowedError":
        message =
          "Microphone access was denied. Please allow microphone access in your browser settings and try again.";
        break;
      case "NotFoundError":
        message =
          "No microphone was found. Please check your microphone connection and try again.";
        break;
      case "NotReadableError":
        message =
          "Could not access your microphone. It may be in use by another application.";
        break;
      case "SecurityError":
        message =
          "Microphone access is not allowed in this context. Please try opening the application in a new tab.";
        break;
      case "InsecureContext":
        message =
          "Microphone access requires a secure context (HTTPS). Please ensure the site is using HTTPS.";
        break;
      case "NotSupported":
        message =
          "Your browser doesn't support microphone access. Please try using a different browser.";
        break;
    }

    toast({
      variant: "destructive",
      title,
      description: message
    });
  };

  // Function to open app in a new browser tab
  const openInNewTab = () => {
    const appUrl = window.location.href;
    window.open(appUrl, "_blank");
  };

  // Function to show Replit-specific instructions
  const showReplitInstructions = () => {
    const instructionsMessage =
      "Audio recording may require additional permissions in Replit's environment. Would you like to:\n\n" +
      "1. Try again with current settings (security headers have been updated)\n" +
      "2. Open in a new browser tab for better microphone access\n\n" +
      "Click 'OK' to open in a new tab, or 'Cancel' to try again here.";

    if (confirm(instructionsMessage)) {
      openInNewTab();
    } else {
      // User chose to try again - show a helpful message
      console.log(
        "[AudioRecorder] User chose to try again in the current environment",
      );
      alert(
        "Please click 'Start Recording' again and accept the microphone permission request when prompted.",
      );
    }
  };

  const handleStartRecording = async () => {
    console.log("[AudioRecorder] Handle start recording triggered");
    
    if (!client) {
      console.error("[AudioRecorder] WebSocket client not initialized");
      toast({
        variant: "destructive",
        title: "Connection Error",
        description: "WebSocket client is not initialized. Please try again in a moment."
      });
      return;
    }
    
    // Unfreeze the AI insights when recording starts to allow new updates
    if (client.eventHandler && client.eventHandler.suggestionsModule) {
      console.log("[AudioRecorder] Unfreezing AI insights on recording start");
      client.eventHandler.suggestionsModule.unfreezeInsights();
      
      // If component has a setAreInsightsFrozen function, call it
      // This is passed from the parent NurseTranscription component
      if (onStartRecording && typeof onStartRecording === 'function') {
        try {
          onStartRecording();
        } catch (error) {
          console.warn("[AudioRecorder] Could not call onStartRecording callback:", error);
        }
      }
    } else {
      console.warn("[AudioRecorder] Could not access suggestionsModule to unfreeze insights");
    }
    
    if (!patientId) {
      console.error("[AudioRecorder] Missing patientId");
      return;
    }
    
    // Enhanced check for API key
    console.log("[AudioRecorder] Client appears to be:", client ? "available" : "null");
    
    // Check for API key availability using server endpoint
    try {
      const response = await fetch('/api/openai-key-check');
      const data = await response.json();
      console.log("[AudioRecorder] OpenAI API key status check:", data);
      
      if (!data.available) {
        console.error("[AudioRecorder] OpenAI API key not available on the server");
        toast({
          variant: "destructive",
          title: "API Key Missing",
          description: "Unable to access AI services. Please check your API key configuration or contact support."
        });
        return;
      }
    } catch (error) {
      console.error("[AudioRecorder] Error checking API key status:", error);
      // Continue anyway, as the endpoint might be unavailable but the key might still work
    }

    // First, check if we're in a secure context and have media device support
    if (!isSecureContext()) {
      console.error("[AudioRecorder] Not in a secure context");
      showPermissionErrorModal("InsecureContext");
      return;
    }

    if (!hasMediaDevices()) {
      console.error("[AudioRecorder] Media devices API not supported");
      showPermissionErrorModal("NotSupported");
      return;
    }

    // Check if we're in a Replit iframe - preemptively fetch microphone permissions
    // This sends a request to our special endpoint that will relax security headers
    if (isReplitIframe()) {
      console.log(
        "[AudioRecorder] Detected Replit iframe environment - fetching explicit microphone permissions",
      );

      // Fetch microphone permissions from our dedicated endpoint
      try {
        fetch("/api/microphone-permission")
          .then((response) => response.json())
          .then((data) => {
            console.log(
              "[AudioRecorder] Microphone permissions response:",
              data,
            );
          })
          .catch((error) => {
            console.error(
              "[AudioRecorder] Error fetching microphone permissions:",
              error,
            );
          });
      } catch (permError) {
        console.error(
          "[AudioRecorder] Error with permissions request:",
          permError,
        );
      }
    }

    try {
      console.log(
        "[AudioRecorder] Environment checks passed, requesting microphone...",
      );

      // Send chart data first before requesting audio permissions
      // This ensures the patient context is sent even if mic permissions fail
      // Convert patientId to string for consistent handling in API calls
      const patientIdStr = patientId.toString();
      
      if (!client.hasPatientChartBeenSent(patientIdStr)) {
        console.log("[AudioRecorder] Fetching patient chart data");
        
        try {
          const chartResponse = await fetch(`/api/patients/${patientIdStr}/chart`);
          
          if (!chartResponse.ok) {
            console.error(`[AudioRecorder] Error fetching chart: ${chartResponse.status}`);
            throw new Error(`Failed to fetch patient chart: ${chartResponse.statusText}`);
          }
          
          const chartData = await chartResponse.json();
          
          // Use our utility function to format chart data for GPT
          const gptSafeChart = formatPatientChartForGpt(chartData);
          
          console.log(
            `[AudioRecorder] SENDING CHART CONTEXT (first time for this session)`,
          );
          const chartContextEvent = {
            type: "conversation.item.create",
            item: {
              id: "system_context",
              type: "message",
              role: "system",
              content: [
                {
                  type: "input_text",
                  text: `This is the patient's chart. It should be used for all subsequent suggestions and responses:\n${JSON.stringify(gptSafeChart, null, 2)}`,
                },
              ],
            },
          };

          // Use the client's sendMessage method instead of directly accessing ws
          client.sendMessage(chartContextEvent);
          // Mark this patient's chart as sent for this session
          client.markPatientChartAsSent(patientIdStr);
        } catch (error) {
          console.error("[AudioRecorder] Error processing chart data:", error);
          // Continue with recording even if chart fails - don't block the user
        }
      } else {
        console.log(
          `[AudioRecorder] SKIPPING CHART CONTEXT (already sent for patient ${patientIdStr})`,
        );
      }

      // Request permission with minimal constraints for Replit environment
      console.log("[AudioRecorder] Requesting basic audio permissions");

      try {
        const stream = await navigator.mediaDevices.getUserMedia({
          audio: {
            echoCancellation: true,
            noiseSuppression: true,
            autoGainControl: true,
            channelCount: 1,
            sampleRate: 24000,
            sampleSize: 16,
          },
        });

        console.log("[AudioRecorder] Basic microphone permission granted");

        // Create audio context with default sample rate for better compatibility
        // ✅ Force AudioContext to match the pcm16 requirement (24kHz)
        const context = new AudioContext({ sampleRate: 24000 });
        console.log(
          `[AudioRecorder] Audio context created with sample rate: ${context.sampleRate}`,
        );

        const source = context.createMediaStreamSource(stream);

        // Use a more compatible buffer size
        const bufferSize = 4096; // Default size that works well across browsers
        const processor = context.createScriptProcessor(
          bufferSize,
          1, // mono input
          1, // mono output
        );

        processor.onaudioprocess = async (e) => {
          if (!client || !client.hasValidApiKey()) {
            console.error("[AudioRecorder] Client not available or has no valid API key");
            onStopRecording();
            toast({
              variant: "destructive",
              title: "API Key Error",
              description: "API key validation failed. Unable to process audio."
            });
            return;
          }
          
          if (!client.isInitialized()) {
            console.log("[AudioRecorder] Client not initialized, attempting to initialize");
            try {
              await client.init();
            } catch (error) {
              console.error("[AudioRecorder] Failed to initialize client:", error);
              onStopRecording();
              toast({
                variant: "destructive",
                title: "Connection Error",
                description: "Failed to initialize WebSocket client. Please try again."
              });
              return;
            }
          }
          
          const inputData = e.inputBuffer.getChannelData(0);
          const blob = processAudioData(
            inputData,
            context.sampleRate,
            e.inputBuffer.numberOfChannels,
          );
          
          try {
            await client.appendAudio(blob);
          } catch (error) {
            console.error("[AudioRecorder] Error sending audio data:", error);
            // Don't stop recording on a single audio buffer failure, just log it
          }
        };

        source.connect(processor);
        processor.connect(context.destination);

        setAudioSource(source);
        setAudioProcessor(processor);
        setAudioContext(context);
        setMediaStream(stream);

        setIsRecording(true);
        console.log("[AudioRecorder] Recording started successfully");
      } catch (micError) {
        console.error("Microphone access error:", micError);

        if (
          micError instanceof DOMException &&
          micError.name === "NotAllowedError"
        ) {
          // If in Replit iframe, suggest opening in a new tab
          if (isReplitIframe()) {
            console.log(
              "[AudioRecorder] Offering to open in new tab or use text mode",
            );
            // First, suggest opening in a new tab
            showReplitInstructions();

            // After that, suggest text mode as a fallback
            setTimeout(() => {
              suggestTextModeIfNeeded();
            }, 500);
          } else {
            showPermissionErrorModal(micError.name);
          }
        } else {
          throw micError; // Re-throw for general error handling
        }
      }
    } catch (error) {
      console.error("Error starting recording:", error);

      // Provide more detailed error information
      if (error instanceof DOMException) {
        console.error(`DOMException (${error.name}): ${error.message}`);
        showPermissionErrorModal(error.name);
      } else if (error instanceof Error) {
        console.error(`Error: ${error.message}`);
        toast({
          variant: "destructive",
          title: "Recording Error",
          description: `Failed to start recording: ${error.message}`
        });
      } else {
        console.error("Unknown error:", error);
        toast({
          variant: "destructive",
          title: "Unknown Error",
          description: "An unknown error occurred while trying to access the microphone."
        });
      }
    }
  };

  const handleStopConfirm = () => {
    setShowConfirmDialog(false);
    setIsRecording(false);

    if (audioContext) {
      audioContext.close();
    }
    if (mediaStream) {
      mediaStream.getTracks().forEach((track) => track.stop());
    }

    // With turn detection disabled, we need to manually commit the audio buffer
    // and create a response when recording stops
    /*
    if (client && client.ws) {
      // Clear any existing buffer first
      client.ws.send(JSON.stringify({ type: "input_audio_buffer.clear" }));

      // Commit the accumulated audio buffer
      client.ws.send(JSON.stringify({ type: "input_audio_buffer.commit" }));

      // Request a response from the model
      // client.ws.send(JSON.stringify({ type: "response.create" }));
    }
*/
    setAudioContext(null);
    setMediaStream(null);
    onStopRecording();
    // handleRecordingTriger();
  };

  const handlePauseResume = () => {
    if (isPaused) {
      if (audioContext && audioProcessor && audioSource) {
        audioContext.resume();
        audioSource.connect(audioProcessor);
        audioProcessor.connect(audioContext.destination);
      }
    } else {
      if (audioProcessor && audioSource) {
        audioSource.disconnect();
        audioProcessor.disconnect();
      }
    }
    setIsPaused(!isPaused);
  };

  // Function to recommend text input after microphone access is repeatedly denied
  const suggestTextModeIfNeeded = () => {
    if (!hasSuggestedTextFallback && isReplitIframe()) {
      hasSuggestedTextFallback = true;
      const fallbackMessage =
        "We've made security updates that should allow microphone access, but if you're still experiencing issues, " +
        "you can use text input mode instead.\n\n" +
        "Would you like to switch to text input mode now? (You can toggle between modes anytime using the 'Text Mode' button)";

      if (confirm(fallbackMessage)) {
        setShowTextMode(true);
        console.log("[AudioRecorder] User switched to text input mode");
      } else {
        console.log("[AudioRecorder] User chose to continue with audio mode");
      }
    }
  };

  // Handle sending text input
  const handleSendText = () => {
    if (!client || !textInput.trim() || !patientId) return;

    // Convert patientId to string for consistent handling in API calls
    const patientIdStr = patientId.toString();

    // Ensure the patient chart has been sent
    if (!client.hasPatientChartBeenSent(patientIdStr)) {
      // We should fetch and send the chart data first
      // This would be similar to what's in handleStartRecording
      fetch(`/api/patients/${patientIdStr}/chart`)
        .then((response) => {
          if (!response.ok) {
            console.error(`[AudioRecorder] Error fetching chart: ${response.status}`);
            throw new Error(`Failed to fetch patient chart: ${response.statusText}`);
          }
          return response.json();
        })
        .then((chartData) => {
          // Use our utility function to format the chart data
          const gptSafeChart = formatPatientChartForGpt(chartData);

          const chartContextEvent = {
            type: "conversation.item.create",
            item: {
              id: "system_context",
              type: "message",
              role: "system",
              content: [
                {
                  type: "input_text",
                  text: `This is the patient's chart. It should be used for all subsequent suggestions and responses:\n${JSON.stringify(gptSafeChart, null, 2)}`,
                },
              ],
            },
          };

          client.sendMessage(chartContextEvent);
          client.markPatientChartAsSent(patientIdStr);

          // Now send the text input
          sendTextMessage();
        })
        .catch((error) => {
          console.error("[AudioRecorder] Error processing chart data for text mode:", error);
          // Continue with sending the text even if chart fetch fails
          sendTextMessage();
        });
    } else {
      // Chart already sent, so just send the text
      sendTextMessage();
    }
  };

  // Helper to actually send the text message
  const sendTextMessage = () => {
    if (!client || !textInput.trim()) return;

    // Format text input as a message
    const textMessage = {
      type: "conversation.item.create",
      item: {
        type: "message",
        role: "user",
        content: [
          {
            type: "input_text",
            text: textInput,
          },
        ],
      },
    };

    client.sendMessage(textMessage);

    // Request a response from the model
    client.sendMessage({ type: "response.create" });

    // Clear the input field
    setTextInput("");
  };

  // Check for Replit iframe outside render to avoid re-computation
  const isInReplitIframe = isReplitIframe();

  return (
    <div className="flex flex-col space-y-4 my-4">
      {/* We've removed the warning banner as we're now allowing direct recording */}

      {/* Audio Controls Row */}
      <div className="flex items-center space-x-2">
        {/* Text Mode Toggle Button */}
        <Button
          variant="outline"
          onClick={() => setShowTextMode(!showTextMode)}
          className="mr-2"
        >
          <MessageSquare className="h-4 w-4 mr-2" />
          {showTextMode ? "Hide Text Mode" : "Text Mode"}
        </Button>

        {/* Recording Button */}
        {!showTextMode && (
          <div className="relative">
            <Button
              variant={isRecording ? "destructive" : "default"}
              onClick={
                isRecording
                  ? () => setShowConfirmDialog(true)
                  : handleStartRecording
              }
              className={cn(
                "relative z-10",
                isRecording && !isPaused && "animate-pulse",
              )}
            >
              {isRecording ? (
                <>
                  <Square className="h-4 w-4 mr-2" />
                  Stop Recording
                </>
              ) : (
                <>
                  <Mic className="h-4 w-4 mr-2" />
                  Start Recording
                </>
              )}
            </Button>
            {isRecording && !isPaused && (
              <>
                <div className="absolute inset-0 rounded-md bg-red-500 animate-ping opacity-75" />
                <div className="absolute -inset-2 rounded-lg bg-red-500 animate-wave opacity-20" />
                <div className="absolute -inset-4 rounded-lg bg-red-500 animate-wave-slow opacity-10" />
              </>
            )}
          </div>
        )}

        {/* Recording Controls */}
        {isRecording && !showTextMode && (
          <>
            <Button variant="outline" onClick={handlePauseResume}>
              {isPaused ? "Resume" : "Pause"} Recording
            </Button>
            <Button
              variant="secondary"
              onClick={() => {
                if (client) {
                  // Commit the accumulated audio buffer
                  client.sendMessage({ type: "input_audio_buffer.commit" });

                  // Request a response from the model
                  client.sendMessage({ type: "response.create" });
                }
              }}
            >
              Generate Response
            </Button>
          </>
        )}
      </div>

      {/* Text Input Area (shown when in text mode) */}
      {showTextMode && (
        <div className="flex w-full space-x-2 items-start">
          <Textarea
            className="flex-grow"
            placeholder="Type your message here instead of using the microphone..."
            value={textInput}
            onChange={(e) => setTextInput(e.target.value)}
            rows={3}
          />
          <Button
            onClick={handleSendText}
            disabled={!textInput.trim()}
            className="mt-1"
          >
            <Send className="h-4 w-4 mr-2" />
            Send
          </Button>
        </div>
      )}

      {/* Stop Recording Confirmation Dialog */}
      <Dialog open={showConfirmDialog} onOpenChange={setShowConfirmDialog}>
        <DialogContent>
          <DialogHeader>
            <DialogTitle>Stop Recording?</DialogTitle>
            <DialogDescription>
              Would you like to stop recording and generate a SOAP note?
            </DialogDescription>
          </DialogHeader>
          <DialogFooter>
            <Button
              variant="outline"
              onClick={() => setShowConfirmDialog(false)}
            >
              Cancel
            </Button>
            <Button variant="default" onClick={handleStopConfirm}>
              Stop and Generate SOAP Note
            </Button>
          </DialogFooter>
        </DialogContent>
      </Dialog>
    </div>
  );
};
