# HYBRID REALTIME + ASSISTANT VOICE WORKFLOW

## ðŸš¨ IMMEDIATE TASK: Build Realtime Transcription + Assistant Suggestions

Build a hybrid system that combines OpenAI Realtime API for live transcription with OpenAI Assistants for intelligent suggestions, SOAP notes, orders, and billing codes.

---

## CURRENT WORKING REALTIME SETUP (PRESERVE THIS EXACTLY)

Your current system has a sophisticated Realtime API implementation that works perfectly in Replit. **DO NOT RECREATE THIS FROM SCRATCH** - use these exact patterns:

### **WebSocket Client Architecture**
```typescript
// Current working WebSocket connection (from WebSocketClient.ts)
const ws = new WebSocket("wss://api.openai.com/v1/realtime", [
  "realtime",
  "openai-beta.realtime-v1",
]);

// Critical: Headers handled by backend proxy for security
// Uses 24kHz PCM16 audio format that works reliably in Replit

class WebSocketClient {
  private ws: WebSocket | null = null;
  private patientChartsSent: Set<string> = new Set();
  
  async init() {
    // Your current working initialization that handles Replit iframe environment
    // Includes microphone permission handling for Replit security context
  }
  
  async appendAudio(audioBlob: ArrayBuffer) {
    // Current working audio streaming that maintains connection stability
  }
}
```

### **Audio Processing (WORKING IMPLEMENTATION)**
```typescript
// From AudioRecorder.tsx - this audio processing works perfectly in Replit
const stream = await navigator.mediaDevices.getUserMedia({
  audio: {
    echoCancellation: true,
    noiseSuppression: true,
    autoGainControl: true,
    channelCount: 1,
    sampleRate: 24000,  // Critical: matches OpenAI requirements
    sampleSize: 16,
  },
});

// Force AudioContext to match the pcm16 requirement (24kHz)
const context = new AudioContext({ sampleRate: 24000 });
const source = context.createMediaStreamSource(stream);
const bufferSize = 4096; // Default size that works well across browsers
const processor = context.createScriptProcessor(bufferSize, 1, 1);

processor.onaudioprocess = async (e) => {
  const inputData = e.inputBuffer.getChannelData(0);
  const blob = processAudioData(inputData, context.sampleRate, 1);
  await client.appendAudio(blob);
};
```

### **Replit-Specific Security Handling**
```typescript
// Critical for Replit iframe environment - your current working solution
function isReplitIframe(): boolean {
  return window.self !== window.top && 
         (window.location.hostname.includes('replit') || 
          window.location.hostname.includes('repl.co'));
}

// Microphone permission handling that works in Replit
if (isReplitIframe()) {
  try {
    fetch("/api/microphone-permission")
      .then((response) => response.json())
      .then((data) => console.log("Microphone permissions response:", data))
      .catch((error) => console.error("Error fetching microphone permissions:", error));
  } catch (permError) {
    console.error("Error with permissions request:", permError);
  }
}
```

---

## NEW HYBRID ARCHITECTURE

**Keep Realtime API for:**
- Live audio streaming and transcription
- Real-time text display as user speaks

**Add Assistants API for:**
- Intelligent suggestions during transcription
- SOAP note generation after recording
- Draft orders and CPT codes
- Historical context and clinical decision support

---

## STEP 1: CREATE Assistant Context Service

```typescript
// services/assistant-context-service.ts
import OpenAI from 'openai';

export class AssistantContextService {
  private openai: OpenAI;
  private assistantId: string;
  
  constructor() {
    this.openai = new OpenAI({
      apiKey: process.env.OPENAI_API_KEY,
    });
    this.assistantId = process.env.OPENAI_ASSISTANT_ID || '';
  }
  
  async initializeAssistant() {
    if (!this.assistantId) {
      const assistant = await this.openai.beta.assistants.create({
        name: "Medical Context Assistant",
        instructions: `You are a medical AI assistant that provides real-time clinical decision support.

REAL-TIME SUGGESTIONS MODE:
When receiving partial transcriptions, provide immediate clinical suggestions:
- For NURSES: Assessment questions, symptom clarification, patient education
- For PROVIDERS: Differential diagnosis, treatment considerations, clinical red flags

HISTORICAL CONTEXT MODE:
Analyze patient history and provide contextual insights:
- Previous encounters and outcomes
- Recurring patterns and chronic conditions
- Relevant test results and medication responses
- Clinical decision continuity

OUTPUT FORMATS:
For real-time suggestions: 
{
  "suggestions": ["immediate suggestion 1", "immediate suggestion 2"],
  "clinicalFlags": ["urgent consideration if applicable"],
  "historicalContext": "relevant past encounter info"
}

For complete processing:
{
  "soapNote": { "subjective": "", "objective": "", "assessment": "", "plan": "" },
  "draftOrders": [{"type": "lab", "details": "CBC with diff", "indication": "anemia workup"}],
  "cptCodes": [{"code": "99213", "description": "Office visit", "units": 1}],
  "chartUpdates": { "historicalUpdates": {}, "factualAppends": {} }
}`,
        model: "gpt-4o",
        tools: []
      });
      this.assistantId = assistant.id;
    }
  }
  
  async getOrCreateThread(patientId: number): Promise<string> {
    // Check if patient already has thread in your new schema
    const patient = await db.select().from(patients)
      .where(eq(patients.id, patientId)).limit(1);
    
    if (patient[0]?.assistantThreadId) {
      return patient[0].assistantThreadId;
    }
    
    // Create new thread with patient context
    const thread = await this.openai.beta.threads.create();
    
    // Load patient's complete medical history
    const patientHistory = await this.getPatientHistory(patientId);
    
    // Add historical context to thread
    await this.openai.beta.threads.messages.create(thread.id, {
      role: "user",
      content: `Patient Historical Context: ${JSON.stringify(patientHistory)}`
    });
    
    // Save thread ID to patient record
    await db.update(patients)
      .set({ assistantThreadId: thread.id })
      .where(eq(patients.id, patientId));
    
    return thread.id;
  }
  
  // Real-time suggestions during transcription
  async getRealtimeSuggestions(
    threadId: string,
    partialTranscription: string,
    userRole: string,
    patientId: number
  ) {
    await this.openai.beta.threads.messages.create(threadId, {
      role: "user",
      content: `PARTIAL TRANSCRIPTION: "${partialTranscription}"
USER ROLE: ${userRole}
MODE: REAL_TIME_SUGGESTIONS

Provide immediate clinical suggestions based on this partial transcription.`
    });
    
    const run = await this.openai.beta.threads.runs.create(threadId, {
      assistant_id: this.assistantId
    });
    
    // Wait for completion (should be fast for suggestions)
    let runStatus = await this.openai.beta.threads.runs.retrieve(threadId, run.id);
    while (runStatus.status === 'in_progress' || runStatus.status === 'queued') {
      await new Promise(resolve => setTimeout(resolve, 500));
      runStatus = await this.openai.beta.threads.runs.retrieve(threadId, run.id);
    }
    
    if (runStatus.status === 'completed') {
      const messages = await this.openai.beta.threads.messages.list(threadId);
      const lastMessage = messages.data[0];
      
      if (lastMessage.content[0].type === 'text') {
        return JSON.parse(lastMessage.content[0].text.value);
      }
    }
    
    return { suggestions: [], clinicalFlags: [], historicalContext: "" };
  }
  
  // Complete processing after recording stops
  async processCompleteTranscription(
    threadId: string,
    fullTranscription: string,
    userRole: string,
    patientId: number,
    encounterId: number
  ) {
    await this.openai.beta.threads.messages.create(threadId, {
      role: "user",
      content: `COMPLETE TRANSCRIPTION: "${fullTranscription}"
USER ROLE: ${userRole}
ENCOUNTER_ID: ${encounterId}
MODE: COMPLETE_PROCESSING

Generate SOAP note, draft orders, CPT codes, and chart updates.`
    });
    
    const run = await this.openai.beta.threads.runs.create(threadId, {
      assistant_id: this.assistantId
    });
    
    // Wait for completion
    let runStatus = await this.openai.beta.threads.runs.retrieve(threadId, run.id);
    while (runStatus.status === 'in_progress' || runStatus.status === 'queued') {
      await new Promise(resolve => setTimeout(resolve, 1000));
      runStatus = await this.openai.beta.threads.runs.retrieve(threadId, run.id);
    }
    
    if (runStatus.status === 'completed') {
      const messages = await this.openai.beta.threads.messages.list(threadId);
      const lastMessage = messages.data[0];
      
      if (lastMessage.content[0].type === 'text') {
        return JSON.parse(lastMessage.content[0].text.value);
      }
    }
    
    throw new Error('Assistant processing failed');
  }
  
  private async getPatientHistory(patientId: number) {
    // Get comprehensive patient data from your new schema
    const [
      patient,
      familyHistory,
      socialHistory,
      allergies,
      recentVitals,
      currentMedications,
      activeDiagnoses,
      recentEncounters
    ] = await Promise.all([
      db.select().from(patients).where(eq(patients.id, patientId)).limit(1),
      db.select().from(familyHistory).where(eq(familyHistory.patientId, patientId)),
      db.select().from(socialHistory).where(eq(socialHistory.patientId, patientId)),
      db.select().from(allergies).where(eq(allergies.patientId, patientId)),
      db.select().from(vitals).where(eq(vitals.patientId, patientId))
        .orderBy(desc(vitals.measuredAt)).limit(10),
      db.select().from(medications).where(eq(medications.patientId, patientId))
        .where(eq(medications.status, 'active')),
      db.select().from(diagnoses).where(eq(diagnoses.patientId, patientId))
        .where(eq(diagnoses.status, 'active')),
      db.select().from(encounters).where(eq(encounters.patientId, patientId))
        .orderBy(desc(encounters.createdAt)).limit(5)
    ]);
    
    return {
      patient: patient[0],
      familyHistory,
      socialHistory,
      allergies,
      recentVitals,
      currentMedications,
      activeDiagnoses,
      recentEncounters
    };
  }
}
```

---

## STEP 2: MODIFY Existing WebSocket Event Handler

```typescript
// Extend your existing WebSocketEventHandler.ts
import { AssistantContextService } from './assistant-context-service.js';

export class WebSocketEventHandler {
  private assistantService: AssistantContextService;
  private currentThreadId: string | null = null;
  private patientId: number | null = null;
  private userRole: string = 'provider';
  
  constructor() {
    this.assistantService = new AssistantContextService();
    // Keep all your existing modules and initialization
  }
  
  async initializePatientContext(patientId: number, userRole: string) {
    this.patientId = patientId;
    this.userRole = userRole;
    this.currentThreadId = await this.assistantService.getOrCreateThread(patientId);
    console.log(`Assistant thread initialized for patient ${patientId}: ${this.currentThreadId}`);
  }
  
  // Keep your existing handleTranscriptionDelta but add Assistant suggestions
  handleTranscriptionDelta(event: any) {
    // Your existing transcription handling code stays the same
    const delta = event.delta;
    if (delta) {
      this.currentTranscription += delta;
      
      // Update UI with live transcription (existing code)
      this.updateTranscriptionDisplay(this.currentTranscription);
      
      // NEW: Get Assistant suggestions for longer transcription chunks
      if (this.currentTranscription.length > 50 && this.currentTranscription.length % 100 === 0) {
        this.getAssistantSuggestions(this.currentTranscription);
      }
    }
  }
  
  private async getAssistantSuggestions(partialTranscription: string) {
    if (!this.currentThreadId || !this.patientId) return;
    
    try {
      const suggestions = await this.assistantService.getRealtimeSuggestions(
        this.currentThreadId,
        partialTranscription,
        this.userRole,
        this.patientId
      );
      
      // Update suggestions display
      this.updateSuggestionsDisplay(suggestions);
    } catch (error) {
      console.error('Error getting Assistant suggestions:', error);
    }
  }
  
  // Keep your existing handleResponseDone but add Assistant processing
  async handleResponseDone(event: any) {
    // Your existing response handling
    console.log("Response completed:", event);
    
    // NEW: Process complete transcription with Assistant
    if (this.currentTranscription && this.currentThreadId && this.patientId) {
      try {
        const completeResults = await this.assistantService.processCompleteTranscription(
          this.currentThreadId,
          this.currentTranscription,
          this.userRole,
          this.patientId,
          this.currentEncounterId
        );
        
        // Apply results to your new schema
        await this.applyCompleteResults(completeResults);
      } catch (error) {
        console.error('Error processing complete transcription:', error);
      }
    }
  }
  
  private async applyCompleteResults(results: any) {
    // Update encounter with SOAP note
    await db.update(encounters)
      .set({
        subjective: results.soapNote.subjective,
        objective: results.soapNote.objective,
        assessment: results.soapNote.assessment,
        plan: results.soapNote.plan,
        draftOrders: results.draftOrders,
        cptCodes: results.cptCodes,
        lastChartUpdate: new Date()
      })
      .where(eq(encounters.id, this.currentEncounterId));
    
    // Apply chart updates using smart update strategy
    if (results.chartUpdates) {
      await this.applySmartChartUpdates(results.chartUpdates);
    }
  }
}
```

---

## STEP 3: ENHANCED Audio Recorder Component

```tsx
// Enhanced AudioRecorder.tsx with Assistant integration
import { AssistantContextService } from '@/services/assistant-context-service';

export function AudioRecorder({ patientId, encounterId, userRole, onSuggestionsUpdate }) {
  const [assistantService] = useState(new AssistantContextService());
  const [threadId, setThreadId] = useState<string | null>(null);
  
  // Keep ALL your existing audio recording logic
  // Add Assistant initialization when recording starts
  
  const handleStartRecording = async () => {
    // Your existing recording start logic stays exactly the same
    // ADD: Initialize Assistant context
    try {
      const newThreadId = await assistantService.getOrCreateThread(patientId);
      setThreadId(newThreadId);
      
      // Initialize WebSocket event handler with Assistant context
      if (client?.eventHandler) {
        await client.eventHandler.initializePatientContext(patientId, userRole);
      }
    } catch (error) {
      console.error('Error initializing Assistant context:', error);
    }
    
    // Continue with your existing recording logic...
  };
  
  // Keep all your existing recording, audio processing, and UI logic
}
```

---

## IMPLEMENTATION PRIORITIES

1. **PRESERVE** your existing Realtime WebSocket setup completely
2. **ADD** AssistantContextService for intelligent suggestions
3. **ENHANCE** WebSocketEventHandler with Assistant integration
4. **MAINTAIN** all existing audio processing and UI components
5. **EXTEND** with Assistant-powered SOAP notes and orders

This gives you the best of both worlds - your working real-time transcription plus intelligent contextual suggestions powered by patient history!