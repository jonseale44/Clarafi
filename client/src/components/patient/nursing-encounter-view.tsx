import { useState, useRef, useEffect } from "react";
import { useQuery, useMutation, useQueryClient } from "@tanstack/react-query";
import { Card, CardContent, CardHeader, CardTitle } from "@/components/ui/card";
import { Button } from "@/components/ui/button";
import { Badge } from "@/components/ui/badge";
import { Textarea } from "@/components/ui/textarea";
import { Tabs, TabsContent, TabsList, TabsTrigger } from "@/components/ui/tabs";
import { Avatar, AvatarFallback, AvatarImage } from "@/components/ui/avatar";
import {
  Collapsible,
  CollapsibleContent,
  CollapsibleTrigger,
} from "@/components/ui/collapsible";
import {
  Mic,
  MicOff,
  Save,
  FileText,
  Heart,
  Activity,
  Shield,
  Clock,
  AlertTriangle,
  CheckCircle,
  User,
  Stethoscope,
  ArrowLeft,
  Search,
  ChevronDown,
  ChevronRight,
} from "lucide-react";
import { useToast } from "@/hooks/use-toast";
import { useAuth } from "@/hooks/use-auth";
import { useSharedRealtimeService } from "@/utils/shared-realtime-service";
import { SharedChartSections } from "@/components/patient/shared-chart-sections";
import { UnifiedChartPanel } from "@/components/patient/unified-chart-panel";

import {
  NursingTemplateAssessment,
  NursingTemplateRef,
} from "@/components/NursingTemplateAssessment";
import { VitalsFlowsheet } from "@/components/vitals/vitals-flowsheet";
import { NursingRecordingPanel } from "@/components/NursingRecordingPanel";

interface NursingTemplateData {
  cc: string;
  hpi: string;
  pmh: string;
  meds: string;
  allergies: string;
  famHx: string;
  soHx: string;
  psh: string;
  ros: string;
  vitals: string;
}
import type { Patient, User as UserType, Encounter } from "@shared/schema";

interface NursingEncounterViewProps {
  patient: Patient;
  encounterId: number;
  onBackToChart: () => void;
}

const nursingChartSections = [
  { id: "encounters", label: "Patient Encounters" },
  { id: "problems", label: "Medical Problems" },
  { id: "medication", label: "Medication" },
  { id: "allergies", label: "Allergies" },
  { id: "vitals", label: "Vitals" },
  { id: "labs", label: "Labs" },
  { id: "imaging", label: "Imaging" },
  { id: "documents", label: "Patient Documents" },
  { id: "family-history", label: "Family History" },
  { id: "social-history", label: "Social History" },
  { id: "surgical-history", label: "Surgical History" },
  { id: "attachments", label: "Attachments" },
  { id: "appointments", label: "Appointments" },
  { id: "nursing-assessments", label: "Nursing Assessments" },
  { id: "care-plans", label: "Care Plans" },
];

export function NursingEncounterView({
  patient,
  encounterId,
  onBackToChart,
}: NursingEncounterViewProps) {
  console.log("ðŸ©º [NursingEncounterView] Component initialized with:", {
    patientId: patient?.id,
    encounterId,
    patientName: patient?.firstName + " " + patient?.lastName,
  });

  const { toast } = useToast();
  const queryClient = useQueryClient();
  const { user, isLoading: authLoading } = useAuth();
  
  console.log("ðŸ©º [NursingEncounterView] Auth state:", {
    user: user?.username,
    role: user?.role,
    authLoading,
  });

  // State management - matching provider view exactly including AI suggestions
  const [isRecording, setIsRecording] = useState(false);
  const [transcription, setTranscription] = useState("");
  const [transcriptionBuffer, setTranscriptionBuffer] = useState("");
  const [liveTranscriptionContent, setLiveTranscriptionContent] = useState("");
  const [gptSuggestions, setGptSuggestions] = useState("");
  const [liveSuggestions, setLiveSuggestions] = useState("");
  const [lastSuggestionTime, setLastSuggestionTime] = useState(0);
  const [suggestionsBuffer, setSuggestionsBuffer] = useState("");
  const [wsConnected, setWsConnected] = useState(false);
  const [templateData, setTemplateData] = useState<NursingTemplateData>({
    cc: "",
    hpi: "",
    pmh: "",
    meds: "",
    allergies: "",
    famHx: "",
    soHx: "",
    psh: "",
    ros: "",
    vitals: "",
  });
  const [expandedSections, setExpandedSections] = useState<Set<string>>(
    new Set(["encounters"]),
  );
  const [isGeneratingInsights, setIsGeneratingInsights] = useState(false);
  const [useRestAPI] = useState(true); // Default to REST API for manual triggering

  const nursingTemplateRef = useRef<NursingTemplateRef>(null);
  const suggestionDebounceTimer = useRef<NodeJS.Timeout | null>(null);
  const audioChunksRef = useRef<Blob[]>([]);

  // Get current user for role-based functionality
  const { data: currentUser } = useQuery({
    queryKey: ["/api/user"],
  }) as { data?: { role?: string } };

  // Get encounter data
  const { data: encounter, isLoading } = useQuery<Encounter>({
    queryKey: [`/api/encounters/${encounterId}`],
    enabled: !!encounterId,
  });
  
  console.log("ðŸ©º [NursingEncounterView] Encounter state:", {
    encounter: encounter?.id,
    isLoading,
    encounterId,
    status: encounter?.encounterStatus,
  });

  // Deduplication tracking for WebSocket messages
  const processedEvents = useRef(new Set<string>());
  const processedContent = useRef(new Set<string>());
  const activeResponseId = useRef<string | null>(null);
  const lastResponseTime = useRef(0);

  const isEventProcessed = (eventId: string) =>
    processedEvents.current.has(eventId);
  const markEventAsProcessed = (eventId: string) =>
    processedEvents.current.add(eventId);
  const isContentProcessed = (content: string) =>
    processedContent.current.has(content);
  const markContentAsProcessed = (content: string) =>
    processedContent.current.add(content);

  // Response management helpers
  const canCreateNewResponse = () => {
    const now = Date.now();
    const timeSinceLastResponse = now - lastResponseTime.current;
    return !activeResponseId.current && timeSinceLastResponse > 3000; // 3 second cooldown
  };
  
  const markResponseActive = (responseId: string) => {
    activeResponseId.current = responseId;
    lastResponseTime.current = Date.now();
  };
  
  const markResponseComplete = () => {
    activeResponseId.current = null;
  };

  // MISSING: Add the startSuggestionsConversation function from provider view
  const startSuggestionsConversation = async (
    ws: WebSocket | null,
    patientData: any,
  ) => {
    if (!ws) return;

    console.log("[NursingView] Starting AI suggestions conversation");

    // 1. Fetch full patient chart data like external implementation
    let patientChart = null;
    try {
      console.log(
        "[NursingView] Fetching patient chart data for context injection",
      );
      const chartResponse = await fetch(
        `/api/patients/${patientData.id}/chart`,
      );
      if (chartResponse.ok) {
        patientChart = await chartResponse.json();
        console.log("[NursingView] Patient chart data fetched successfully");
      } else {
        console.warn(
          "[NursingView] Failed to fetch patient chart, continuing without context",
        );
      }
    } catch (error) {
      console.error("[NursingView] Error fetching patient chart:", error);
    }

    // 2. Build comprehensive patient context with enhanced chart sections for nursing AI
    const patientContext = `Patient: ${patientData.firstName} ${patientData.lastName}
Age: ${patientData.dateOfBirth ? new Date().getFullYear() - new Date(patientData.dateOfBirth).getFullYear() : "Unknown"}
Gender: ${patientData.gender || "Unknown"}
MRN: ${patientData.mrn || "Unknown"}

${
  patientChart
    ? `
MEDICAL PROBLEMS:
${patientChart.medicalProblems?.length > 0 
  ? patientChart.medicalProblems.map((p: any) => `- ${p.problemTitle} (${p.problemStatus})`).join("\n")
  : "- No active medical problems documented"
}

CURRENT MEDICATIONS:
${patientChart.currentMedications?.length > 0 
  ? patientChart.currentMedications.map((m: any) => `- ${m.medicationName} ${m.dosage} ${m.frequency}`).join("\n")
  : "- No current medications documented"
}

ALLERGIES:
${patientChart.allergies?.length > 0 
  ? patientChart.allergies.map((a: any) => `- ${a.allergen}: ${a.reaction} (${a.severity})`).join("\n")
  : "- NKDA (No Known Drug Allergies)"
}

RECENT VITALS:
${patientChart.vitals?.length > 0 
  ? `- BP: ${patientChart.vitals[0].systolic}/${patientChart.vitals[0].diastolic} mmHg
- HR: ${patientChart.vitals[0].heartRate} bpm
- Temp: ${patientChart.vitals[0].temperature}Â°F
- RR: ${patientChart.vitals[0].respiratoryRate || "Not recorded"}
- O2 Sat: ${patientChart.vitals[0].oxygenSaturation || "Not recorded"}%`
  : "- No recent vitals available"
}

FAMILY HISTORY:
${patientChart.familyHistory?.length > 0
  ? patientChart.familyHistory.map((fh: any) => `- ${fh.relationship}: ${fh.condition}`).join("\n")
  : "- No significant family history documented"
}

SOCIAL HISTORY:
${patientChart.socialHistory?.length > 0
  ? patientChart.socialHistory.map((sh: any) => `- ${sh.category}: ${sh.details}`).join("\n")
  : "- Social history not documented"
}

SURGICAL HISTORY:
${patientChart.surgicalHistory?.length > 0
  ? patientChart.surgicalHistory.map((sh: any) => `- ${sh.procedure} (${sh.date})`).join("\n")
  : "- No surgical history documented"
}
`
    : "Limited patient data available - chart context not accessible"
}`;

    // 3. Send patient context to OpenAI for AI suggestions
    const currentTranscription =
      liveTranscriptionContent || transcriptionBuffer || "";

    const contextWithTranscription = `${patientContext}

CURRENT LIVE CONVERSATION:
${currentTranscription}

CRITICAL: If the nurse is asking direct questions about patient chart information, provide SPECIFIC facts from the chart data above, NOT generic suggestions.

Examples:
- Question: "Does the patient have medical problems?" â†’ Answer: "Medical problems: HTN, DM2, CKD stage 3, AFib, CHF"
- Question: "What medications?" â†’ Answer: "Current medications: Acetaminophen 500mg daily"
- Question: "Any allergies?" â†’ Answer: "NKDA (No Known Drug Allergies)"

DO NOT say "Confirm" or "Assess" - give the actual chart facts directly.

Please provide nursing insights based on the current conversation.`;

    const contextMessage = {
      type: "conversation.item.create",
      item: {
        type: "message",
        role: "user",
        content: [
          {
            type: "input_text",
            text: contextWithTranscription,
          },
        ],
      },
    };

    console.log(
      "ðŸ§  [NursingView] Injecting patient context for AI suggestions",
    );
    ws.send(JSON.stringify(contextMessage));

    // 4. Create response for AI suggestions with metadata like external system
    if (canCreateNewResponse()) {
      const suggestionsMessage = {
        type: "response.create",
        response: {
          modalities: ["text"],
          instructions: `You are a medical AI assistant for nursing staff. ALWAYS RESPOND IN ENGLISH ONLY, regardless of what language is used for input. NEVER respond in any language other than English under any circumstances. Provide concise, single-line medical insights for nurses.

CRITICAL PRIORITY: When nurses ask direct questions about patient information, provide SPECIFIC factual answers using the chart data provided in the conversation context. Do NOT give generic advice when asked direct questions.

DIRECT QUESTION RESPONSES:
  -When nurse asks "Does patient have medical problems?" â†’ Answer: "Medical problems: HTN, DM2, CKD stage 3, AFib, CHF with reduced EF"
  -When nurse asks "What medications?" â†’ Answer: "Current medications: Acetaminophen 500mg once daily by mouth"
  -When nurse asks "Any allergies?" â†’ Answer: "NKDA (No Known Drug Allergies)"
  -FORBIDDEN responses: "Confirm...", "Assess...", "Obtain details..." when chart data exists

Focus on high-value, evidence-based, nursing assessments and safety considerations based on what the patient is saying in this conversation. Provide only one brief phrase at a time. If multiple insights could be provided, prioritize the most critical or relevant one first.

Avoid restating general knowledge or overly simplistic recommendations a nurse would already know. Prioritize specifics: vital signs monitoring, medication safety, patient comfort measures, and nursing interventions. Avoid explanations or pleasantries. Stay brief and actionable. Limit to one insight per response.

DO NOT WRITE IN FULL SENTENCES, JUST BRIEF PHRASES.

IMPORTANT: Return only 1-2 insights maximum per response. Use a bullet (â€¢), dash (-), or number to prefix each insight. Keep responses short and focused.

Format each bullet point on its own line with no extra spacing between them.`,
          metadata: {
            type: "suggestions",
          },
        },
      };

      console.log("ðŸ§  [NursingView] Creating AI suggestions conversation");
      markResponseActive("suggestions_initial");
      ws.send(JSON.stringify(suggestionsMessage));
    } else {
      console.log("ðŸ§  [NursingView] Skipping response creation - active response exists");
    }
  };

  // Generate smart suggestions function - EXACT COPY from provider view
  const generateSmartSuggestions = () => {
    setGptSuggestions(
      "AI-generated nursing suggestions based on the encounter...",
    );
    toast({
      title: "Smart Suggestions Generated",
      description: "GPT analysis complete",
    });
  };

  // Manual AI insights generation function for nursing
  const generateNursingInsights = async () => {
    if (!transcription || isGeneratingInsights) return;
    
    console.log("ðŸ§  [NursingView] Generating nursing insights manually");
    setIsGeneratingInsights(true);
    try {
      const response = await fetch("/api/voice/live-suggestions", {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({
          patientId: patient.id.toString(),
          userRole: "nurse",
          transcription: transcription,
          isLiveChunk: "false" // Full transcription for manual trigger
        })
      });
      
      if (response.ok) {
        const data = await response.json();
        console.log("âœ… [NursingView] Nursing insights generated:", data.suggestions);
        // Accumulate new insights
        setLiveSuggestions(prev => prev + "\n" + data.suggestions);
        setGptSuggestions(prev => prev + "\n" + data.suggestions);
      }
    } catch (error) {
      console.error("âŒ [NursingView] Error generating insights:", error);
      toast({
        title: "Error",
        description: "Failed to generate nursing insights",
        variant: "destructive"
      });
    } finally {
      setIsGeneratingInsights(false);
    }
  };

  // Function to get real-time suggestions during recording - EXACT COPY from provider view
  const getLiveAISuggestions = async (transcription: string) => {
    if (transcription.length < 15) return; // Process smaller chunks for faster response

    // Debounce suggestions to prevent too many rapid API calls
    const now = Date.now();
    if (now - lastSuggestionTime < 1000) {
      // Clear any existing timeout and set a new one
      if (suggestionDebounceTimer.current) {
        clearTimeout(suggestionDebounceTimer.current);
      }
      suggestionDebounceTimer.current = setTimeout(() => {
        getLiveAISuggestions(transcription);
      }, 1000);
      return;
    }

    setLastSuggestionTime(now);

    try {
      console.log(
        "ðŸ§  [NursingView] Getting live AI suggestions for transcription:",
        transcription.substring(0, 100) + "...",
      );

      const requestBody = {
        patientId: patient.id.toString(),
        userRole: "nurse",
        isLiveChunk: "true",
        transcription: transcription,
      };

      console.log(
        "ðŸ§  [NursingView] Sending live suggestions request to /api/voice/live-suggestions",
      );
      console.log("ðŸ§  [NursingView] Request body:", requestBody);

      const response = await fetch("/api/voice/live-suggestions", {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
        },
        body: JSON.stringify(requestBody),
      });

      console.log(
        "ðŸ§  [NursingView] Live suggestions response status:",
        response.status,
      );
      console.log(
        "ðŸ§  [NursingView] Live suggestions response headers:",
        Object.fromEntries(response.headers.entries()),
      );

      if (response.ok) {
        const data = await response.json();
        console.log("ðŸ§  [NursingView] Live AI suggestions received:", data);

        if (data.aiSuggestions) {
          console.log(
            "ðŸ”§ [NursingView] Processing live suggestions, existing:",
            liveSuggestions?.length || 0,
            "chars",
          );
          console.log(
            "ðŸ”§ [NursingView] New suggestions count:",
            data.aiSuggestions.realTimePrompts?.length || 0,
          );

          // Get existing live suggestions to append to them (like transcription buffer)
          const existingLiveSuggestions = liveSuggestions || "";
          let suggestionsText = existingLiveSuggestions;

          // If this is the first suggestion, add the header
          if (
            !existingLiveSuggestions.includes("ðŸ©º REAL-TIME CLINICAL INSIGHTS:")
          ) {
            suggestionsText = "ðŸ©º REAL-TIME CLINICAL INSIGHTS:\n\n";
            console.log("ðŸ”§ [NursingView] First suggestion - added header");
          }

          // Only append new suggestions if they exist and aren't empty
          if (data.aiSuggestions.realTimePrompts?.length > 0) {
            // Filter out empty suggestions and ones already in the buffer
            const newSuggestions = data.aiSuggestions.realTimePrompts.filter(
              (prompt: string) => {
                if (
                  !prompt ||
                  !prompt.trim() ||
                  prompt.trim() === "Continue recording for more context..."
                ) {
                  return false;
                }
                // Check if this suggestion is already in our accumulated text
                const cleanPrompt = prompt.replace(/^[â€¢\-\*]\s*/, "").trim();
                return !existingLiveSuggestions.includes(cleanPrompt);
              },
            );

            if (newSuggestions.length > 0) {
              console.log(
                "ðŸ”§ [NursingView] Adding",
                newSuggestions.length,
                "new suggestions to existing",
                existingLiveSuggestions.length,
                "chars",
              );

              // Append each new suggestion (like transcription delta accumulation)
              newSuggestions.forEach((prompt: string) => {
                const formattedPrompt =
                  prompt.startsWith("â€¢") ||
                  prompt.startsWith("-") ||
                  prompt.startsWith("*")
                    ? prompt
                    : `â€¢ ${prompt}`;
                suggestionsText += `${formattedPrompt}\n`;
                console.log(
                  "ðŸ”§ [NursingView] Accumulated suggestion:",
                  formattedPrompt.substring(0, 50) + "...",
                );
              });
            } else {
              console.log("ðŸ”§ [NursingView] No new suggestions to accumulate");
            }
          }

          console.log(
            "ðŸ”§ [NursingView] Final live suggestions length:",
            suggestionsText.length,
          );
          setLiveSuggestions(suggestionsText);
          setGptSuggestions(suggestionsText); // Also update the display
          console.log(
            "ðŸ”§ [NursingView] Live suggestions updated, length:",
            suggestionsText.length,
          );
        }
      } else {
        const errorText = await response.text();
        console.error(
          "âŒ [NursingView] Live suggestions HTTP error:",
          response.status,
        );
        console.error("âŒ [NursingView] Full HTML response:", errorText);
        throw new Error(
          `HTTP ${response.status}: ${errorText.substring(0, 200)}`,
        );
      }
    } catch (error) {
      console.error("âŒ [NursingView] Live suggestions failed:", error);
      console.error("âŒ [NursingView] Error details:", {
        message: (error as any)?.message,
        name: (error as any)?.name,
        stack: (error as any)?.stack,
      });
    }
  };

  // Start recording using same OpenAI Realtime API as provider view
  const startRecording = async () => {
    console.log(
      "ðŸŽ¤ [NursingView] Starting REAL-TIME voice recording for patient:",
      patient.id,
    );
    console.log("ðŸŽ¤ [NursingView] Current state at start:", {
      encounter: encounter?.id,
      templateData,
      isRecording,
      patient: patient?.id,
      currentUser: currentUser?.role,
    });

    // Clear previous transcription and suggestions when starting new recording
    setGptSuggestions("");
    setLiveSuggestions(""); // Clear live suggestions for new encounter
    setSuggestionsBuffer(""); // Clear suggestions buffer for fresh accumulation
    setTranscription("");
    setTranscriptionBuffer("");
    setLiveTranscriptionContent("");

    try {
      // Create direct WebSocket connection to OpenAI like provider view
      let realtimeWs: WebSocket | null = null;
      let transcriptionBuffer = "";

      try {
        console.log("ðŸŒ [NursingView] Connecting to secure WebSocket proxy...");

        // Use WebSocket proxy for secure API key handling - MATCH PROVIDER VIEW EXACTLY
        const params = new URLSearchParams({
          patientId: patient.id.toString(),
          encounterId: encounterId.toString(),
        });

        const protocol = window.location.protocol === 'https:' ? 'wss:' : 'ws:';
        const wsUrl = `${protocol}//${window.location.host}/api/realtime/connect?${params.toString()}`;
        
        console.log("ðŸ”§ [NursingView] Creating secure WebSocket connection through proxy:", wsUrl);
        console.log("ðŸ”§ [NursingView] - Patient ID:", patient.id);
        console.log("ðŸ”§ [NursingView] - Encounter ID:", encounterId);
        
        realtimeWs = new WebSocket(wsUrl);
        // Store WebSocket reference in window to avoid undefined reference
        (window as any).nursingRealtimeWs = realtimeWs;

        realtimeWs.onopen = () => {
          console.log("ðŸŒ [NursingView] âœ… Connected to secure WebSocket proxy");
          setWsConnected(true);
          
          // Session configuration: Match provider view structure exactly
          const sessionConfig = {
            model: "gpt-4o-realtime-preview-2024-10-01",
            modalities: ["text", "audio"],
            instructions: `You are a medical transcription assistant specialized in nursing documentation. 
              Accurately transcribe medical terminology, drug names, dosages, and clinical observations. 
              Translate all languages into English. Only output ENGLISH.
              Pay special attention to:
              - Medication names, dosages, and administration times
              - Vital signs and measurements
              - Patient complaints and symptoms
              - Nursing assessments and interventions
              Format with bullet points for natural conversation flow.`,
            input_audio_format: "pcm16",
            input_audio_transcription: {
              model: "whisper-1",
              language: "en",
            },
            turn_detection: {
              type: "server_vad",
              threshold: 0.5,
              prefix_padding_ms: 300,
              silence_duration_ms: 300,
              create_response: false,
            },
          };

          // Send session creation request matching provider view format exactly
          const sessionCreateMessage = {
            type: "session.create",
            data: {
              patientId: patient.id,
              encounterId: encounterId,
              sessionConfig: sessionConfig,
            },
          };

          console.log("ðŸ“¤ [NursingView] Session creation request:");
          console.log(JSON.stringify(sessionCreateMessage, null, 2));

          realtimeWs!.send(JSON.stringify(sessionCreateMessage));
          
          // After session is created, trigger response creation for transcription
          setTimeout(() => {
            if (realtimeWs && realtimeWs.readyState === WebSocket.OPEN) {
              console.log("ðŸŽ¯ [NursingView] Triggering response creation for transcription");
              const responseCreateMessage = {
                type: "response.create",
                response: {
                  modalities: ["text"],
                  instructions: "Please transcribe the audio input."
                }
              };
              realtimeWs.send(JSON.stringify(responseCreateMessage));
            }
          }, 1000);
        };

        realtimeWs.onmessage = (event) => {
          const message = JSON.parse(event.data);
          console.log("ðŸ“¨ [NursingView] WebSocket message received:", {
            type: message.type,
            hasData: !!message.data,
            hasError: !!message.error,
            hasTranscript: !!message.transcript,
            hasDelta: !!message.delta,
            fullMessage: message
          });

          // Handle session creation response first
          if (message.type === "session.created") {
            console.log("âœ… [NursingView] Session created successfully");
            console.log("ðŸŽ¤ [NursingView] Session details:", message);
            console.log("ðŸŽ¤ [NursingView] Ready to receive transcriptions");
          }
          
          // Handle session updates
          else if (message.type === "session.updated") {
            console.log("ðŸ”„ [NursingView] Session updated:", message);
          }
          
          // Handle errors
          else if (message.type === "error") {
            console.error("âŒ [NursingView] WebSocket error:", message.error);
            const errorMessage = typeof message.error === 'object' 
              ? message.error.message || JSON.stringify(message.error)
              : message.error;
            console.error("âŒ [NursingView] Error details:", errorMessage);
            console.error("âŒ [NursingView] Full error message:", message);
          }

          // Handle transcription delta messages
          else if (message.type === "audio.transcription.delta") {
            const deltaText = message.delta || "";
            console.log("ðŸ“ [NursingView] Transcription delta received:", {
              delta: deltaText,
              currentBuffer: transcriptionBuffer,
              messageDetails: message
            });
            
            // Accumulate transcription
            transcriptionBuffer += deltaText;
            setTranscriptionBuffer(transcriptionBuffer);
            setLiveTranscriptionContent(transcriptionBuffer);
            setTranscription(transcriptionBuffer);
            
            console.log("ðŸ“ [NursingView] Updated transcription buffer:", transcriptionBuffer);
          }
          
          // Handle transcription completion  
          else if (message.type === "conversation.item.input_audio_transcription.completed") {
            const finalText = message.transcript || "";
            console.log("âœ… [NursingView] Transcription completed:", {
              transcript: finalText,
              messageDetails: message
            });
            
            // Update the transcription with the final text
            if (finalText.trim()) {
              setTranscription(finalText);
              setLiveTranscriptionContent(finalText);
              setTranscriptionBuffer(finalText);
              transcriptionBuffer = finalText;
              console.log("âœ… [NursingView] Final transcription set:", finalText);
            }
          }
          
          // Handle any response creation
          else if (message.type === "response.created") {
            console.log("ðŸ¤– [NursingView] Response created:", message);
          }
          
          // Skip AI suggestion messages - handled via REST API only
          else if (message.type === "response.text.delta" || message.type === "response.text.done") {
            console.log("ðŸ¤– [NursingView] AI response message (skipping):", message.type);
            return;
          }
          
          // Log any unhandled message types
          else {
            console.warn("âš ï¸ [NursingView] Unhandled message type:", {
              type: message.type,
              message: message
            });
          }
        };
        
        // Setup WebSocket error handling
        realtimeWs.onerror = (error) => {
          console.error("âŒ [NursingView] WebSocket error:", error);
        };
        
        realtimeWs.onclose = () => {
          console.log("ðŸ”Œ [NursingView] WebSocket connection closed");
          setWsConnected(false);
        };

        console.log("âœ… [NursingView] WebSocket setup complete");

        // Set up audio recording
        console.log("ðŸŽ¤ [NursingView] Requesting microphone access...");
        const stream = await navigator.mediaDevices.getUserMedia({
          audio: {
            echoCancellation: true,
            noiseSuppression: true,
            autoGainControl: true,
            sampleRate: 16000,
          },
        });
        console.log("ðŸŽ¤ [NursingView] âœ… Microphone access granted");

        // Set up audio processing
        const audioContext = new AudioContext({ sampleRate: 16000 });
        const source = audioContext.createMediaStreamSource(stream);
        const processor = audioContext.createScriptProcessor(4096, 1, 1);

        // Set up MediaRecorder
        const mediaRecorder = new MediaRecorder(stream);
        
        // Audio chunk counter for debugging
        let audioChunksSent = 0;
        let lastLogTime = Date.now();
        
        // Process audio data and send to WebSocket
        processor.onaudioprocess = (e) => {
          if (!isRecording || !realtimeWs) {
            if (!isRecording) console.log("ðŸŽ¤ [NursingView] Audio processor: Not recording");
            if (!realtimeWs) console.log("ðŸŽ¤ [NursingView] Audio processor: No WebSocket");
            return;
          }

          const inputData = e.inputBuffer.getChannelData(0);
          const pcm16 = new Int16Array(inputData.length);
          
          // Check if we have actual audio data
          let hasAudio = false;
          for (let i = 0; i < inputData.length; i++) {
            const sample = Math.max(-1, Math.min(1, inputData[i]));
            pcm16[i] = sample < 0 ? sample * 0x8000 : sample * 0x7FFF;
            if (Math.abs(sample) > 0.001) hasAudio = true;
          }
          
          const audioData = pcm16.buffer;
          if (audioData.byteLength > 0 && hasAudio) {
            const base64Audio = btoa(
              String.fromCharCode.apply(null, Array.from(new Uint8Array(audioData)))
            );
            
            const audioMessage = {
              type: "input_audio_buffer.append",
              audio: base64Audio,
            };
            
            try {
              if (realtimeWs.readyState === WebSocket.OPEN) {
                realtimeWs.send(JSON.stringify(audioMessage));
                audioChunksSent++;
                
                // Log every second
                const now = Date.now();
                if (now - lastLogTime > 1000) {
                  console.log("ðŸŽµ [NursingView] Audio status:", {
                    chunksSent: audioChunksSent,
                    wsState: realtimeWs.readyState,
                    wsStateText: ['CONNECTING', 'OPEN', 'CLOSING', 'CLOSED'][realtimeWs.readyState],
                    audioDataSize: audioData.byteLength,
                    hasAudio: hasAudio
                  });
                  lastLogTime = now;
                }
              } else {
                console.warn("âš ï¸ [NursingView] WebSocket not open:", {
                  state: realtimeWs.readyState,
                  stateText: ['CONNECTING', 'OPEN', 'CLOSING', 'CLOSED'][realtimeWs.readyState]
                });
              }
            } catch (error) {
              console.error("âŒ [NursingView] Error sending audio:", error);
            }
          }
        };
        
        source.connect(processor);
        processor.connect(audioContext.destination);
        
        // Handle MediaRecorder data
        mediaRecorder.ondataavailable = (event) => {
          if (event.data.size > 0 && audioChunksRef.current) {
            audioChunksRef.current.push(event.data);
          }
        };
        
        mediaRecorder.onstop = () => {
          console.log("ðŸŽ¤ [NursingView] MediaRecorder stopped");
        };
        
        // Store MediaRecorder reference and start recording
        (window as any).currentMediaRecorder = mediaRecorder;
        mediaRecorder.start(100);
        
        setIsRecording(true);
        console.log("âœ… [NursingView] Recording started successfully");
        
      } catch (error) {
        console.error("âŒ [NursingView] Failed to start recording:", error);
        setIsRecording(false);
      }
    } catch (error) {
      console.error("âŒ [NursingView] Failed to setup recording:", error);
      setIsRecording(false);
    }
  };

  const stopRecording = async () => {
    console.log("ðŸŽ¤ [NursingView] === STOP RECORDING CALLED ===");
    
    // Stop the MediaRecorder
    const mediaRecorder = (window as any).currentMediaRecorder;
    if (mediaRecorder && mediaRecorder.state === "recording") {
      console.log("ðŸŽ¤ [NursingView] Stopping MediaRecorder...");
      mediaRecorder.stop();
    }
    
    // Close WebSocket connection
    const realtimeWs = (window as any).nursingRealtimeWs;
    if (realtimeWs && realtimeWs.readyState === WebSocket.OPEN) {
      console.log("ðŸ”Œ [NursingView] Closing WebSocket connection...");
      realtimeWs.close();
      (window as any).nursingRealtimeWs = null;
    }
    
    setIsRecording(false);
    console.log("âœ… [NursingView] Recording stopped successfully");
  };

  // Component UI render
  if (!user && !authLoading) {
    return (
      <div className="flex items-center justify-center h-screen">
        <p className="text-red-500">Not authenticated. Please log in.</p>
      </div>
    );
  }

  if (authLoading || isLoading) {
    return (
      <div className="flex items-center justify-center h-screen">
        <p className="text-gray-500">Loading...</p>
      </div>
    );
  }

  return (
    <div className="flex h-full">
      {/* Left Chart Panel - Unified Expandable */}
      <UnifiedChartPanel
        patient={patient}
        config={{
          context: 'nurse-encounter',
          userRole: currentUser?.role,
          allowResize: true,
          defaultWidth: "w-80",
          maxExpandedWidth: "90vw",
          enableSearch: true
        }}
        encounterId={encounterId}
        encounter={encounter}
        onBackToChart={onBackToChart}
      />

      {/* Main Content */}
      <div className="flex-1 flex flex-col overflow-hidden">
        {/* Header */}
        <div className="px-8 py-4 border-b border-gray-200 bg-white">
          <div className="flex items-center justify-between">
            <div className="flex-1 flex items-center">
              <h1 className="text-xl font-semibold text-gray-900">
                Nursing Assessment
              </h1>
            </div>
          </div>
        </div>

        {/* Content Area - Two Panel Layout */}
        <div className="flex-1 flex gap-4 overflow-hidden p-4">
          {/* Left Panel - Nursing Template */}
          <div className="flex-1 overflow-y-auto">
            <div className="max-w-4xl mx-auto space-y-6">
              <div className="space-y-6">
                {/* Comprehensive Nursing Template */}
                <NursingTemplateAssessment
                  ref={nursingTemplateRef}
                  patientId={patient.id.toString()}
                  encounterId={encounterId.toString()}
                  isRecording={isRecording}
                  transcription={liveTranscriptionContent || transcription}
                  onTemplateUpdate={(data) => {
                    setTemplateData(data);
                    console.log("ðŸ©º [NursingView] Template updated:", data);
                  }}
                  autoStart={false}
                />
              </div>
            </div>
          </div>

          {/* Right Panel - Recording and Suggestions */}
          <div className="w-96 overflow-y-auto">
            <NursingRecordingPanel
              isRecording={isRecording}
              transcription={liveTranscriptionContent || transcription}
              aiSuggestions={liveSuggestions || gptSuggestions}
              wsConnected={wsConnected}
              onStartRecording={startRecording}
              onStopRecording={stopRecording}
              onGenerateInsights={generateNursingInsights}
              isGeneratingInsights={isGeneratingInsights}
            />
          </div>
        </div>
      </div>
    </div>
  );
};
